{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT-MultiRAGNet\n",
    "\n",
    "| Paper Block                        | Implemented As                              |\n",
    "| ---------------------------------- | ------------------------------------------- |\n",
    "| Patch Partition + Linear Embedding | ViT patch embedding (timm)                  |\n",
    "| Transformer Encoder Stack          | ViT encoder (L layers)                      |\n",
    "| CLS Token                          | `feats[:,0]`                                |\n",
    "| Clinical Data (8-D vector)         | Metadata → MLP                              |\n",
    "| Clinical Embedding & Query Gen     | Image CLS + Clinical embedding              |\n",
    "| Knowledge Base                     | FAISS feature memory                        |\n",
    "| Top-K Retrieval                    | FAISS `search()`                            |\n",
    "| Cross-Attention RAG Fusion         | `nn.MultiheadAttention`                     |\n",
    "| Final Fusion                       | Residual fusion                             |\n",
    "| Classification Head                | MLP                                         |\n",
    "| Segmentation Decoder               | U-Net-style decoder                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d65dd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.4.1 which is incompatible.\n",
      "langchain 0.1.16 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.3.79 which is incompatible.\n",
      "langchain 0.1.16 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.42 which is incompatible.\n",
      "langchain 0.1.16 requires numpy<2,>=1, but you have numpy 2.4.1 which is incompatible.\n",
      "langchain 0.1.16 requires tenacity<9.0.0,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.79 which is incompatible.\n",
      "langchain-community 0.0.38 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.42 which is incompatible.\n",
      "langchain-community 0.0.38 requires numpy<2,>=1, but you have numpy 2.4.1 which is incompatible.\n",
      "langchain-community 0.0.38 requires tenacity<9.0.0,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.4.1 which is incompatible.\n",
      "numba 0.58.0 requires numpy<1.26,>=1.21, but you have numpy 2.4.1 which is incompatible.\n",
      "pandas 2.2.1 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.4.1 which is incompatible.\n",
      "qiskit-aer 0.17.1 requires qiskit>=1.1.0, but you have qiskit 0.45.0 which is incompatible.\n",
      "qiskit-terra 0.45.0 requires numpy<2,>=1.17, but you have numpy 2.4.1 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.4.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q timm faiss-cpu albumentations opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61034f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _swigfaiss: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27976\\4229164.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# STAGE 1 — IMPORTS & CONFIG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\faiss\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# We import * so that the symbol foo can be accessed as faiss.foo.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# additional wrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfaiss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclass_wrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\faiss\\loader.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;31m# we import * so that the symbol X can be accessed as faiss.X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading faiss.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mswigfaiss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Successfully loaded faiss.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Python 2.7 or later required\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Import the low-level C/C++ module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__package__\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _swigfaiss: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# STAGE 1 — IMPORTS & CONFIG\n",
    "import os, cv2, torch, faiss, timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LR = 1e-4\n",
    "BASE = \"k_CBIS-DDSM\"  # Update this path to your dataset location\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 2 — LOAD CSV (CBIS-DDSM Dataset)\n",
    "calc = pd.read_csv(f\"{BASE}/calc_case(with_jpg_img).csv\")\n",
    "mass = pd.read_csv(f\"{BASE}/mass_case(with_jpg_img).csv\")\n",
    "df = pd.concat([calc, mass]).reset_index(drop=True)\n",
    "df = df[df[\"jpg_fullMammo_img_path\"].notna()]\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Pathology distribution:\\n{df['pathology'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 3 — DATASET (IMAGE + CLINICAL VECTOR)\n",
    "class CBISDataset(Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tf = A.Compose([\n",
    "            A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            A.HorizontalFlip(p=0.5 if augment else 0),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "\n",
    "        img = cv2.imread(r[\"jpg_fullMammo_img_path\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load ROI mask if available\n",
    "        if isinstance(r[\"jpg_ROI_img_path\"], str) and os.path.exists(r[\"jpg_ROI_img_path\"]):\n",
    "            mask = cv2.imread(r[\"jpg_ROI_img_path\"], 0)\n",
    "        else:\n",
    "            mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "        t = self.tf(image=img, mask=mask)\n",
    "        img, mask = t[\"image\"], t[\"mask\"].unsqueeze(0).float() / 255.0\n",
    "\n",
    "        # 8-D Clinical vector (normalized)\n",
    "        clinical = torch.tensor([\n",
    "            r[\"assessment\"] / 5,\n",
    "            r[\"subtlety\"] / 5,\n",
    "            r[\"breast_density\"] / 4,\n",
    "            1 if r[\"image view\"] == \"MLO\" else 0,\n",
    "            1 if r[\"left or right breast\"] == \"LEFT\" else 0,\n",
    "            1 if r[\"abnormality type\"] == \"mass\" else 0,\n",
    "            0,  # Placeholder for additional clinical features\n",
    "            0   # Placeholder for additional clinical features\n",
    "        ], dtype=torch.float)\n",
    "\n",
    "        label = torch.tensor(1 if \"MALIGNANT\" in r[\"pathology\"].upper() else 0)\n",
    "\n",
    "        return img, mask, clinical, label\n",
    "\n",
    "print(\"Dataset class defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 4 — ViT ENCODER (PATCH + TRANSFORMER STACK)\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Block: Patch Partition + Linear Embedding + Transformer Encoder Stack\n",
    "    Uses pretrained ViT-Base with 16x16 patches\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(\n",
    "            \"vit_base_patch16_224\",\n",
    "            pretrained=True,\n",
    "            num_classes=0  # Remove classification head\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Returns all patch embeddings including CLS token\n",
    "        # Shape: (B, 197, 768) -> 196 patches + 1 CLS token\n",
    "        return self.vit.forward_features(x)\n",
    "\n",
    "print(\"ViT Encoder defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 5 — RAG MEMORY (KNOWLEDGE BASE)\n",
    "class RAGMemory:\n",
    "    \"\"\"\n",
    "    Paper Block: Knowledge Base + Top-K Retrieval\n",
    "    FAISS-based feature memory for retrieval-augmented generation\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=768):\n",
    "        self.dim = dim\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.store = []\n",
    "\n",
    "    def add(self, feats):\n",
    "        \"\"\"Add feature vectors to the knowledge base\"\"\"\n",
    "        if isinstance(feats, torch.Tensor):\n",
    "            feats = feats.cpu().numpy()\n",
    "        feats = np.ascontiguousarray(feats.astype(np.float32))\n",
    "        self.index.add(feats)\n",
    "        self.store.extend(feats)\n",
    "\n",
    "    def query(self, q, k=5):\n",
    "        \"\"\"Retrieve top-k similar features\"\"\"\n",
    "        if isinstance(q, torch.Tensor):\n",
    "            q = q.cpu().numpy()\n",
    "        q = np.ascontiguousarray(q.astype(np.float32))\n",
    "        _, idx = self.index.search(q, k)\n",
    "        \n",
    "        # Handle case where store might not have enough items\n",
    "        results = []\n",
    "        for row in idx:\n",
    "            row_feats = []\n",
    "            for i in row:\n",
    "                if i >= 0 and i < len(self.store):\n",
    "                    row_feats.append(self.store[i])\n",
    "                else:\n",
    "                    row_feats.append(np.zeros(self.dim, dtype=np.float32))\n",
    "            results.append(row_feats)\n",
    "        \n",
    "        return torch.tensor(np.array(results), dtype=torch.float32)\n",
    "\n",
    "    def size(self):\n",
    "        return self.index.ntotal\n",
    "\n",
    "print(\"RAG Memory defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eec22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 6 — CROSS-ATTENTION RAG FUSION (CORE OF PAPER)\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Block: Cross-Attention RAG Fusion\n",
    "    Query = Image CLS + Clinical embedding\n",
    "    Key/Value = Retrieved features from knowledge base\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        # q: (B, dim) -> (B, 1, dim)\n",
    "        # kv: (B, K, dim) where K = top-k retrieved features\n",
    "        out, _ = self.attn(q.unsqueeze(1), kv, kv)\n",
    "        return out.squeeze(1)  # (B, dim)\n",
    "\n",
    "print(\"Cross-Attention module defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 7 — ViT-MultiRAGNet (FINAL MODEL)\n",
    "class ViTMultiRAGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper-faithful implementation of ViT-MultiRAGNet\n",
    "    - ViT Encoder for image feature extraction\n",
    "    - Clinical data MLP for metadata embedding  \n",
    "    - Cross-attention RAG fusion with knowledge base\n",
    "    - Dual heads: Classification + Segmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = ViTEncoder()\n",
    "        self.clin_fc = nn.Linear(8, 768)  # Clinical data embedding\n",
    "        self.cross_attn = CrossAttention(768)\n",
    "        \n",
    "        # Classification head (MLP)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Segmentation decoder (U-Net style)\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, clinical, rag_feats):\n",
    "        # ViT encoding\n",
    "        feats = self.encoder(x)  # (B, 197, 768)\n",
    "        cls_token = feats[:, 0]  # CLS token (B, 768)\n",
    "        \n",
    "        # Clinical embedding\n",
    "        clinical_emb = self.clin_fc(clinical)  # (B, 768)\n",
    "        \n",
    "        # Query generation: CLS + Clinical\n",
    "        query = cls_token + clinical_emb\n",
    "        \n",
    "        # Cross-attention RAG fusion\n",
    "        fused = self.cross_attn(query, rag_feats)  # (B, 768)\n",
    "        \n",
    "        # Residual fusion\n",
    "        fused = fused + query\n",
    "        \n",
    "        # Classification output\n",
    "        cls_out = self.cls_head(fused)\n",
    "        \n",
    "        # Segmentation output\n",
    "        B, N, C = feats.shape\n",
    "        h = w = int(np.sqrt(N - 1))  # 14x14 for ViT-Base\n",
    "        fmap = feats[:, 1:].permute(0, 2, 1).reshape(B, C, h, w)\n",
    "        seg_out = self.seg_head(fmap)\n",
    "        seg_out = F.interpolate(seg_out, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return seg_out, cls_out\n",
    "\n",
    "print(\"ViT-MultiRAGNet model defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 8 — DATA PREPARATION & TRAINING SETUP\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"pathology\"], random_state=42)\n",
    "train_loader = DataLoader(CBISDataset(train_df, augment=True), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(CBISDataset(val_df, augment=False), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ff80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 8B — MODEL & OPTIMIZER INITIALIZATION\n",
    "model = ViTMultiRAGNet().to(DEVICE)\n",
    "rag = RAGMemory()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "seg_criterion = nn.BCEWithLogitsLoss()\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model initialized on {DEVICE}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcbce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 8C — BUILD KNOWLEDGE BASE (POPULATE RAG MEMORY)\n",
    "print(\"Building knowledge base from training data...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, _, clin, _ in tqdm(train_loader, desc=\"Building RAG Memory\"):\n",
    "        img = img.to(DEVICE)\n",
    "        feats = model.encoder(img)[:, 0]  # CLS token features\n",
    "        rag.add(feats.cpu().numpy())\n",
    "\n",
    "print(f\"Knowledge base populated with {rag.size()} feature vectors ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 9 — TRAINING LOOP\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for img, mask, clin, y in pbar:\n",
    "        img = img.to(DEVICE)\n",
    "        mask = mask.to(DEVICE)\n",
    "        clin = clin.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        # Query RAG memory with current features\n",
    "        with torch.no_grad():\n",
    "            q = model.encoder(img)[:, 0].detach().cpu().numpy()\n",
    "        rag_feats = rag.query(q).to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        seg_out, cls_out = model(img, clin, rag_feats)\n",
    "        \n",
    "        # Compute losses\n",
    "        seg_loss = seg_criterion(seg_out, mask)\n",
    "        cls_loss = cls_criterion(cls_out, y)\n",
    "        loss = seg_loss + cls_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        pred = cls_out.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.1f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Train Acc = {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 10 — VALIDATION & EVALUATION\n",
    "print(\"Running validation...\")\n",
    "model.eval()\n",
    "preds, gts = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, _, clin, y in tqdm(val_loader, desc=\"Validating\"):\n",
    "        img = img.to(DEVICE)\n",
    "        clin = clin.to(DEVICE)\n",
    "        \n",
    "        q = model.encoder(img)[:, 0].cpu().numpy()\n",
    "        rag_feats = rag.query(q).to(DEVICE)\n",
    "        \n",
    "        _, cls_out = model(img, clin, rag_feats)\n",
    "        preds.extend(cls_out.argmax(1).cpu().numpy())\n",
    "        gts.extend(y.numpy())\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(gts, preds)*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(gts, preds, target_names=['BENIGN', 'MALIGNANT']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(gts, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
